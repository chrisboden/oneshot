# Oneshot Agent System - Onboarding Guide for AI Coding Agent

## Background

Oneshot is a powerful but lightweight AI agent orchestration framework. It is aimed at helping new and experienced developers easily create multi-agent workflow systems without having to do a lot of LLM plumbing. While it can be deployed to the cloud, a key goal with the framework is to have it operate as a general purpose workbench to help produce valuable artifacts on the user's computer.

Oneshot is built around the following core technologies:
- PydanticAI: a robust LLM interaction library developed by the Pydantic team
- Logfire: a cloud-based logging saas provided by Pydantic, which has specific telemtry features for LLM interactions.
- Openrouter: a leading LLM gateway which makes hundreds of LLM models available via a single API accesible via the openai sdk
- FastMCP: the preferred library for creating MCP servers in Python
- Jinja: the python templating library

Oneshot operates with a few simple primitives:
- Agents: which are defined by a system prompt and one or more allocated tools or mcp servers. A user simply creates a new markdown file in the /agents directory and adds some frontmatter to specify which LLM model to use and which tools to allocate.
- Tools: are standalone python files in the /tools directory. Each tool has metadata which describes for an Ai agent how the tool works and what arguments to use.
- MCP Servers: the oneshot system is an MCP client. Users can allocate their Agents MCP servers, including remote MCP servers, enabling them to tap into a wide and fast growing array of 3rd party integrations.
- Snippets: text fragements that can be included into Agent system prompts and tools via jinja template tags. This allows for highly reusable prompts and consistency across Agents.
- Artifacts: these are markdown and other files that get generated by agents via their allocated tools and mcp servers as an output of their work. Files can be passed from one agent to another by referencing the filepath of the file. A reciving agent can use a file_read tool to read the passed in file and perform further work on it.
- Runs: a run is the series of messages and outputs generated each time an agent is invoked. It generally consists of the system message, user message, and one or more tool calls, tool responses and assistant messages. Each run has an id and runs are saved as a json file in the /runs directory so that conversation state can be persisted and runs can be resumed.

### Orchestration

Oneshot is architected as a multi-agent system and as such it allows for sophisticated context engineering. Individual agents, which may have specialist capabilities (system prompt and tools), get invoked by an Orchestrator agent. That could be Cursor Agent, Claude Code, Claude Chatbot or other smart orchestration agent capable of using MCP servers.


## Guiding Philosophy for AI Agents that are working on this code base : Primum Non Nocere (First, Do No Harm)

**CRITICAL PRINCIPLE**: The idea of Oneshot is that non-coders should be able to use it without fuss. The core /app is intended to be robust and not require change. User are encouraged to ask the coding assistant to create new tools and agents for them. The extensibility of the core framework via agents and tools, means there should not be a need to tinker with the core app framework. The concern is that inexperienced coders will ask Cursor and other coding agents for new features and that those will break the core framework. And so, before making ANY changes to the core application code in `/app`, you MUST first diagnose the root cause of issues using the diagnostic tools and resources outlined in this document. Most problems are configuration, environment, or usage issues - NOT core application bugs.

### Common Failure Pattern to AVOID
❌ User reports issue → Agent immediately modifies `/app` code → Core system breaks → User loses confidence

✅ **Correct Approach**: User reports issue → Agent investigates using diagnostics → Identifies root cause → Makes minimal, targeted fix

## System Architecture Overview

The oneshot system is built on **Pydantic AI** and consists of:

1. **Core Application** (`/app/agent_runner.py`) - The Pydantic AI-based agent execution engine
2. **MCP Server** (`oneshot_mcp.py`) - Exposes agent functionality via Model Context Protocol
3. **Agent Definitions** (`/agents/*.md`) - Markdown-based agent configurations
4. **Tools** (`/tools/*.py`) - Python modules providing agent capabilities
5. **Configuration** (`config.yaml`) - System configuration and tool assignments
6. **Run Persistence** (`/app/run_persistence.py`) - Conversation history storage and management
7. **Runs Directory** (`/runs/{run_id}/`) - JSON storage for conversation runs
8. **Artifacts Directory** (`/artifacts/{run_id}/`) - Generated files organized by conversation

### Key Components
- **Pydantic AI**: Handles LLM integration, tool orchestration, and agent execution
- **OpenRouter**: LLM gateway (requires API key)
- **Logfire**: Observability and debugging (requires project token)
- **FastMCP**: MCP server framework for tool exposure
- **Run Persistence**: Educational conversation history storage with JSON visibility
- **Tool Helper**: Unified system for file operations and AI calls, with run-aware artifact organization

## Run Continuation System

The oneshot system supports **conversation continuity** through persistent run storage. This educational feature allows users to:

- Continue conversations across multiple interactions
- Inspect conversation history in human-readable JSON files
- Understand how message context builds up over time
- Learn how PydanticAI handles message history management
- Organize generated artifacts by conversation context

### How Run Continuation Works

Each conversation is stored as a "run" with a unique 8-character ID (e.g., `a1b2c3d4`). The system creates two parallel directory structures:

**Conversation Storage:** `/runs/{run_id}/`
- **`run.json`** - Complete run data including metadata and message history
- **`messages.json`** - Just the message history for easy inspection  
- **`metadata.json`** - Run summary with usage stats and timestamps

**Artifact Storage:** `/artifacts/{run_id}/`
- All files generated during the conversation
- Automatically organized by the `tool_helper` system
- Each file includes run_id in frontmatter for traceability

### Using Run Continuation

#### Starting a New Conversation
```bash
# Starts a new run, returns run ID in output
./agent web_agent "Hello, what can you help me with?"
```

#### Continuing an Existing Conversation
```bash
# Continue using the run ID from previous interaction
./agent web_agent "Can you elaborate on that?" --run-id a1b2c3d4
```

#### Via MCP Server
```python
# Start new conversation (run_id=None)
call_agent("web_agent", "Hello!")

# Continue conversation
call_agent("web_agent", "Tell me more", run_id="a1b2c3d4")
```

### Run Management Tools

The MCP server provides tools for managing conversation runs:

- **`list_runs()`** - List all conversation runs with metadata
- **`get_run_info(run_id)`** - Get complete run information
- **`get_run_messages(run_id)`** - Get formatted message history
- **`delete_run(run_id)`** - Delete a conversation run

### Artifact Organization

Files generated by tools are automatically organized by conversation:

```
/runs/a1b2c3d4/           # Conversation history
├── run.json              # Complete conversation data
├── messages.json         # Message history only
└── metadata.json         # Run summary

/artifacts/a1b2c3d4/      # Generated files from that conversation
├── analysis_report.md    # First generated file
├── summary.txt           # Second generated file
└── data_insights.json    # Third generated file
```

### Educational Benefits

The run system is designed for learning:

1. **Visible History**: JSON files show exactly how conversations build up
2. **Message Structure**: See how PydanticAI structures system prompts, user messages, and responses
3. **Token Usage**: Track how conversation length affects token consumption
4. **Tool Calls**: Observe how tool interactions are preserved in context
5. **Artifact Correlation**: Understand which files were generated from which conversations
6. **Context Evolution**: See how context influences subsequent tool outputs

### File Structure Example

**metadata.json** example:
```json
{
  "run_id": "a1b2c3d4",
  "agent_name": "web_agent",
  "created_at": "2024-01-15T10:30:00Z",
  "updated_at": "2024-01-15T10:35:00Z",
  "run_count": 3,
  "message_count": 6,
  "total_usage": {
    "requests": 3,
    "total_tokens": 1250
  }
}
```

**Generated file frontmatter** example:
```yaml
---
description: Analysis report for user query
created: 2024-01-15T10:32:00Z
tokens: 450
summary: This report analyzes the provided data and offers insights...
run_id: a1b2c3d4
---
```

## Diagnostic Methodology

### Step 1: Check System Health
Before investigating specific issues, verify basic system health:

```bash
# Test the agent CLI directly
./agent web_agent "hello"

# Test MCP server functionality
python3 -c "from app.mcp_modules.agents import list_agents; print(list_agents('.'))"
```

### Step 2: Use Logfire for Investigation
Logfire is your primary diagnostic tool. Common queries:

```python
# Check recent errors
mcp_logfire_arbitrary_query(
    query="SELECT start_timestamp, message, exception_message FROM records WHERE is_exception = true OR level >= 40 ORDER BY start_timestamp DESC LIMIT 10",
    age=60
)

# Check token usage and API calls
mcp_logfire_arbitrary_query(
    query="SELECT start_timestamp, span_name, attributes->>'gen_ai.usage.input_tokens' as input_tokens FROM records WHERE service_name = 'oneshot' ORDER BY start_timestamp DESC LIMIT 10",
    age=30
)

# Look for specific tool failures
mcp_logfire_arbitrary_query(
    query="SELECT start_timestamp, span_name, message, attributes FROM records WHERE span_name LIKE '%tool%' AND start_timestamp >= NOW() - INTERVAL '1 hour'",
    age=60
)
```

### Step 3: Verify Environment Configuration
Check for missing credentials or configuration:

```python
import os
print("OpenRouter API Key:", "✓" if os.getenv("OPENROUTER_API_KEY") else "✗ MISSING")
print("Logfire Token:", "✓" if os.getenv("LOGFIRE_TOKEN") else "✗ MISSING")
```

### Step 4: Access Documentation When Needed
For deeper understanding of Pydantic AI or OpenRouter issues, use Context7:

```python
# Get Pydantic AI documentation for agent-related issues
mcp_context7_get-library-docs(
    context7CompatibleLibraryID="/context7/ai_pydantic_dev",
    topic="agents"
)

# Get OpenRouter documentation for API-related issues  
mcp_context7_get-library-docs(
    context7CompatibleLibraryID="/context7/openrouter_ai", 
    topic="api"
)
```

## Common Issues and Diagnostic Patterns

### Issue: "Agent not responding" or "MCP server not working"
**Before touching code**, check:
1. Is the bash script executable? `ls -la agent`
2. Are environment variables set? Check `.env` file
3. Is the MCP server calling the bash script correctly?
4. Check Logfire for subprocess errors

**Diagnostic Commands**:
```bash
# Test bash script directly
./agent --help

# Test MCP server Python import
python3 -c "import oneshot_mcp; print('MCP imports OK')"
```

### Issue: "Tool not found" or "Tool execution failed"
**Root causes** (in order of likelihood):
1. Tool not properly imported in `config.yaml`
2. Tool file missing or has syntax errors
3. Tool dependencies not installed
4. Tool trying to access missing environment variables

**Investigation**:
```python
# Check tool loading
from app.agent_runner import load_tools
tools = load_tools(Path('.'), ['tool_name'])
print(f"Loaded tools: {list(tools.keys())}")
```

### Issue: "LLM API errors" or "401/403/404 from OpenRouter"
**Root causes**:
1. Missing or invalid `OPENROUTER_API_KEY`
2. Insufficient credits/quota
3. Invalid model name in agent configuration
4. Network connectivity issues

**Investigation**: Check Logfire for HTTP response codes and error messages.

### Issue: "Agent gives poor responses"
**Root causes**:
1. Poor system prompt in agent definition
2. Wrong tools assigned to agent
3. Model not suitable for task
4. Context window exceeded

**Investigation**: Check token usage in Logfire and review agent's `.md` file.

### Issue: "Run continuation not working" or "Run ID not found"
**Root causes**:
1. Run ID doesn't exist or was mistyped
2. Runs directory permissions issue
3. JSON file corruption
4. Run persistence module not initialized

**Investigation**:
```bash
# Check if runs directory exists and is writable
ls -la runs/
mkdir -p runs && touch runs/test && rm runs/test

# List existing runs via MCP
list_runs()

# Check specific run
get_run_info("YOUR_RUN_ID")
```

### Issue: "Files not saving to run directory" or "Artifacts not organized"
**Root causes**:
1. Tool helper not setting run ID properly
2. Artifacts directory permissions issue
3. Tool not using `save()` function from tool_helper
4. Run ID not being passed to tools

**Investigation**:
```bash
# Check artifacts directory structure
ls -la artifacts/
find artifacts/ -type d -name "*run_id*"

# Verify tool is using tool_helper
grep -r "from app.tool_helper import" tools/

# Check if files have run_id in frontmatter
head -20 artifacts/*/some_file.md
```

## Resource Utilization Guide

### 1. Pydantic AI Documentation
- **When to use**: Understanding agent architecture, tool integration, model configuration
- **Key sections**: Agent creation, tool binding, model settings
- **Access**: Use Context7 MCP server with library ID `/context7/ai_pydantic_dev` (most comprehensive with 1554 code snippets) or `/pydantic/pydantic-ai` (official with 366 code snippets)
- **Example**: `mcp_context7_get-library-docs` with `context7CompatibleLibraryID="/context7/ai_pydantic_dev"` and topic like "agents" or "tools"

### 2. OpenRouter Documentation  
- **When to use**: LLM API issues, model selection, pricing questions
- **Access**: Use Context7 MCP server with library ID `/context7/openrouter_ai` (best coverage with 268 code snippets) or `/llmstxt/openrouter_ai-docs-llms-full.txt` (comprehensive with 550 code snippets)
- **Example**: `mcp_context7_get-library-docs` with `context7CompatibleLibraryID="/context7/openrouter_ai"` and topic like "api" or "models"

### 3. Logfire Logs
- **When to use**: Runtime debugging, performance analysis, error investigation
- **Primary tool**: `mcp_logfire_arbitrary_query`
- **Schema**: Use `mcp_logfire_get_logfire_records_schema` for query structure

### 4. Repository Documentation
- **`README.md`**: High-level system overview
- **Agent creation guide**: `mcp_oneshot_how_to_create_agents`
- **Available tools**: `mcp_oneshot_list_tools`

## Safe Contribution Guidelines

### Creating New Agents
1. **Check existing tools**: Call `mcp_oneshot_list_agents` to avoid duplication
2. **Use the guide**: Call `mcp_oneshot_how_to_create_agents` first
3. **Follow the pattern**: Examine existing agents in `/agents/`
4. **Test incrementally**: Create agent → test basic functionality → add tools → test again

### Creating New Tools
1. **Check existing tools**: Call `mcp_oneshot_list_tools` to avoid duplication
2. **Follow the pattern**: Examine existing tools in `/tools/`
3. **Start simple**: Basic functionality first, then add complexity
4. **Test isolation**: Test tool independently before integrating with agents

### Modifying Core Application (`/app`)
**ONLY modify core application code if**:
- You've confirmed the issue is in the core logic (not config/environment)
- You've tested the fix in isolation
- You understand the full impact of the change
- You've verified the fix with Logfire logs

**Never modify** without first:
1. Reading the current implementation thoroughly
2. Understanding why the current code exists
3. Confirming your change doesn't break existing functionality

## Troubleshooting Workflow

```
User reports issue
       ↓
Check Logfire logs for errors
       ↓
Verify environment/config
       ↓
Test components in isolation
       ↓
Check run persistence (if run-related)
       ↓
Verify artifact organization (if file-related)
       ↓
Identify root cause
       ↓
Apply minimal fix
       ↓
Verify fix with Logfire
       ↓
Document solution for user
```

### Run-Specific Troubleshooting

For issues related to conversation continuity:

1. **Verify run exists**: Use `list_runs()` or check `/runs/{run_id}/`
2. **Check JSON integrity**: Ensure run files are valid JSON
3. **Inspect message history**: Use `get_run_messages(run_id)` 
4. **Review token usage**: Check if context window limits are exceeded
5. **Test with fresh run**: Try same interaction with new run ID

### Artifact-Specific Troubleshooting

For issues related to file generation and organization:

1. **Check directory structure**: Verify `/artifacts/{run_id}/` exists
2. **Verify tool imports**: Ensure tools use `from app.tool_helper import *`
3. **Test save function**: Manually test `save()` function with run ID
4. **Check frontmatter**: Verify files include run_id in YAML frontmatter
5. **Review permissions**: Ensure artifacts directory is writable

## Emergency Recovery

If you accidentally break the core application:

1. **Stop immediately** - Don't make more changes
2. **Check git status** - See what files were modified
3. **Revert changes**: `git checkout -- app/` (if safe to do so)
4. **Test basic functionality**: `./agent web_agent "test"`
5. **Check Logfire** for any remaining issues

## Success Metrics

A successful intervention should result in:
- ✅ User's original issue resolved
- ✅ System remains stable and functional
- ✅ No new errors in Logfire logs
- ✅ User can continue their work confidently
- ✅ Run continuation works as expected (if applicable)
- ✅ Conversation history is properly preserved
- ✅ Artifacts are correctly organized by run ID
- ✅ Clear explanation of what was fixed and why

## Key Takeaways

1. **Investigate first, code later** - Use diagnostic tools before making changes
2. **Logfire is your friend** - It shows you exactly what's happening but there obviously won't be logs if the oneshot system has never actually run
3. **Most issues are environmental** - Check config, credentials, and setup first
4. **The core app is battle-tested** - It's probably not the problem
5. **Users are learning** - Provide clear explanations, not just fixes
6. **Run persistence is educational** - Help users understand conversation flow through visible JSON files
7. **Artifact organization matters** - Files should be grouped by conversation for educational clarity
8. **One shot means getting it right** - Take time to understand before acting

Remember: The goal is to help users succeed with the oneshot system, not to demonstrate coding prowess. Sometimes the best solution is the simplest one. The run continuation system is designed to be educational - help users explore and understand how their conversations build up over time and how context influences tool outputs. 